{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpIPMNPoU_-p"
   },
   "source": [
    "# How do machines understand language?\n",
    "\n",
    "© Explore Data Science Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Gain a basic understanding of text cleaning;\n",
    "- Understand how to extract features from text.\n",
    "\n",
    "## Outline\n",
    "In this train we will cover:\n",
    "- The NLTK Library\n",
    "- The MBTI Dataset\n",
    "- Text Cleaning\n",
    "    - Removing Noise\n",
    "    - Tokenisation\n",
    "    - Stemming \n",
    "    - Lemmatisation\n",
    "    - Stop Words  \n",
    "- Text Feature Extraction \n",
    "    - n-grams\n",
    "    - Bag of words \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial introduces basic concepts in Natural Language Processing. Particularly common techniques for handling, processing and preparing unstructured text data for use with machine learning models. The concepts introduced here are also very useful for text analysis, so please feel free to do more research and see if you can implement these with the MBTI dataset.\n",
    "\n",
    "Before we get started, lets get the data and the main library we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKmhMwlDU_-u"
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubXxX-njU_-u"
   },
   "source": [
    "NLTK - Natural Language Toolkit - is a leading library for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
    "\n",
    "Let's import `nltk` and other packages to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQm0O5XHU_-z",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# set plot style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C68jzOFpU_-2"
   },
   "source": [
    "Some of the `nltk` text processing methods introduced in this train involve a lookup operation. For example, to find all [stopwords](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/) in a given string of text, we require a list of all possible stopwords in the English language to use for the lookup. Such a list is refered to as a [corpus](https://en.wikipedia.org/wiki/Text_corpus). Therefore, we need to first download the corpora we're going use, otherwise we may get a lookup error! Watch out specifically for the `tokenize` and `stopwords` sections. Not to worry, as we can easily avoid these errors by downloading the [corpora](http://www.nltk.org/nltk_data/) using the `nltk` downloader tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29356,
     "status": "error",
     "timestamp": 1560340175121,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "w8Iw1yCRU_-2",
    "outputId": "188501d1-fcf6-45be-8a45-56f885491dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lct3bK9aU_-7"
   },
   "source": [
    "You should see this pop-up box. \n",
    "\n",
    "**NOTE:** the box might pop-up in the backround, in which case you should use `alt + tab` to switch to the downloader window.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/nltk_downloader.png?raw=true\" width=50%/> \n",
    "\n",
    "Use it to navigate to the item we need to download: \n",
    "- stopwords corpus (Corpora tab)\n",
    "- punkt tokenizer models (Models tab)\n",
    "\n",
    "Navigate to these, click the download button, and exit the downloader when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhoRTAwgU_-7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# or you can download directly, i.e.\n",
    "#nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the download was successful, then the following import should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we'll explore stopwords in greater detail later in this train, it won't hurt to take a quick look at what we've downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MBTI dataset\n",
    "\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides people into one of 16 distinct personality types across 4 axes:\n",
    "\n",
    "    Introversion (I) – Extroversion (E)\n",
    "    Intuition (N) – Sensing (S)\n",
    "    Thinking (T) – Feeling (F)\n",
    "    Judging (J) – Perceiving (P)\n",
    "\n",
    "[(More can be learned about what these mean here)](https://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/home.htm)\n",
    "\n",
    "So for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/MyersBriggsTypes.png'>Image by <a href=\"//commons.wikimedia.org/wiki/User:JakeBeech\" title=\"User:JakeBeech\">Jake Beech</a> - <span class=\"int-own-work\" lang=\"en\"></span><a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>\n",
    "\n",
    "In this train, we'll use a version of [the MBTI dataset](https://www.kaggle.com/datasnaek/mbti-type) which contains over 6000 rows of data, on each row is a person’s:\n",
    "\n",
    " - Type (This person's 4 letter MBTI code)\n",
    " - A section of each of the last 50 things they have posted online (Each entry separated by \"|||\" (3 pipe characters))      \n",
    " \n",
    "_**Note:** If you are curious, you can find out what your MBTI personality is by taking the test here: https://www.16personalities.com/_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKeKPQWkU_-9"
   },
   "source": [
    "### Let's get the data and clean it up a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "error",
     "timestamp": 1560340182462,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "1g8hjHxFU_-9",
    "outputId": "07b8ba22-a4e3-4d73-c121-55cd4a827b10"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')\n",
    "mbti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll print off a list of all the MBTI personality types which are present in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n"
     ]
    }
   ],
   "source": [
    "type_labels = list(mbti.type.unique())\n",
    "print(type_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDSv8UyDU__G"
   },
   "source": [
    "Let's have a look at how many data samples we have for each of the different MBTI personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3G_aB4hU__G",
    "outputId": "92110f2c-5cab-4f7b-a5b3-4a9b0be103e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7h-hcmsU__J"
   },
   "source": [
    "It looks like we have very few samples for the 'ES' types. Maybe because they are out in the real-world, not sitting behind a computer screen! :)   \n",
    "   \n",
    "Let's increase the size of the dataset by separating each of the 50 posts in the `posts` column of each row into its own row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88DRAzHnU__J",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hF5aiupbU__M",
    "outputId": "897367e2-ffbb-4ced-f418-54ed1178e646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316548, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many rows do we have now?\n",
    "all_mbti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bElSlHKGU__W",
    "outputId": "182b0f14-6348-49f7-ff80-63515865a30a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnKe3gyoU__Z"
   },
   "source": [
    "The proportions of the classes remain the same, but we now have a much larger number of samples for the 'ES' personality types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCXae5QXU__Z"
   },
   "source": [
    "## Text Cleaning\n",
    "\n",
    "### Removing Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwJMs441U__a"
   },
   "source": [
    "In text analytics, removing noise (i.e. unneccesary information) is a key part of getting the data into a usable format.  Some techniques are standard, but your own data will require some creative thinking on your part.\n",
    "\n",
    "For the MBTI dataset we will be doing the following steps:\n",
    "* removing the web-urls\n",
    "* making everything lower case\n",
    "* removing punctuation\n",
    "\n",
    "**[Regular expressions](https://www.regular-expressions.info/)** can be very useful for extracting information from text.  If you feel brave, go teach yourself all about it... If not, just follow along.  This next step effectively removes all websites and replaces them with the text `'web-url'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwQ40KKDU__a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLOKzwsdU__d",
    "outputId": "04f9009e-4ec0-4e6d-91a5-9f5fb59b497f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'url-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>url-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  url-web  sportscenter n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>url-web   url-web  On repeat for most of today.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post\n",
       "0  INFJ                                           'url-web\n",
       "1  INFJ                                            url-web\n",
       "2  INFJ  enfp and intj moments  url-web  sportscenter n...\n",
       "3  INFJ  What has been the most life-changing experienc...\n",
       "4  INFJ    url-web   url-web  On repeat for most of today."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-anTzPrnU__h"
   },
   "source": [
    "**Seed of an idea...** There seem to be a lot of YouTube and other links embedded.  Maybe you can think of ways to collect even more information from these links?  How about page titles and names of Youtube videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzM8TbWBU__h"
   },
   "source": [
    "### Remove punctuation\n",
    "\n",
    "First we make all the text lower case to remove some noise from capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUATZmo5U__h",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the punctuation using the `string` import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvhXxcHzU__j",
    "outputId": "7d82c9f5-e448-4f1c-edf9-c75493ea7cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-RPGgE5U__l",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sFFdyNCU__n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just when i think i’ve lost you just when i’m so tired i toss away the fight and say “i’ll just embrace my demons then… ‘cause you feel so far away and i’ll never be your angel” —that’s when'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)\n",
    "all_mbti['post'].iloc[268558]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some punctuation snuck through! See if you can figure out why? Hint it has something to do with the standard encoding on text files in python..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvA-QZmRU__r"
   },
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRDkqnGgU__s"
   },
   "source": [
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\" (see the [Stanford Tokeniser](https://nlp.stanford.edu/software/tokenizer.html)). We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9nnIjyhU__t",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stR6cyC_U__v",
    "outputId": "2c2ed231-656c-420f-dd04-fe19d8df1d09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'tokenizer',\n",
       " 'divides',\n",
       " 'text',\n",
       " 'into',\n",
       " 'a',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'tokens',\n",
       " ',',\n",
       " 'which',\n",
       " 'roughly',\n",
       " 'correspond',\n",
       " 'to',\n",
       " '``',\n",
       " 'words',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TreeBankWordTokenizer since it is MUCH quicker than the word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2rChKdjU__y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8o8u2kwU__1",
    "outputId": "cf9aad76-1846-4af1-aef1-399bd4a31edc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'find',\n",
       " 'all',\n",
       " 'of',\n",
       " 'you',\n",
       " 'to',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'humorous',\n",
       " 'now',\n",
       " 'to',\n",
       " 'find',\n",
       " 'other',\n",
       " 'specimen',\n",
       " 'to',\n",
       " 'observe']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti['tokens'].iloc[55555]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAUkklVXU__6"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlw3iqz3U__7"
   },
   "source": [
    "Stemming is the process of transforming to the root word. It uses an algorithm that removes\n",
    "common word-endings from English words, such as “ly,” “es,” “ed,” and “s.” \n",
    "\n",
    "For example, assuming for an analysis you may want to consider “carefully,” “cared,” “cares,” “caringly” as “care” instead of separate words. There are three widely used stemming algorithms, namely:\n",
    "* Porter\n",
    "* Lancaster\n",
    "* Snowball\n",
    "\n",
    "Out of these three, we will be using the `SnowballStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRY-EGeuU__7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5hmBs_DU__-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "words = 'caring cares cared caringly carefully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILU4MAgaVAAA",
    "outputId": "e012205b-1a2a-441a-f614-1dd1ee498ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n",
      "care\n",
      "care\n",
      "care\n",
      "care\n"
     ]
    }
   ],
   "source": [
    "# find the stem of each word in words\n",
    "stemmer = SnowballStemmer('english')\n",
    "for word in words.split():\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us stem all of the words in the MBTI dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iw9106FVAAE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mbti_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjTuPQNOVAAF"
   },
   "outputs": [],
   "source": [
    "all_mbti['stem'] = all_mbti['tokens'].apply(mbti_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print off the results of the stemmer to see what we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i                    --> i         \n",
      "hate                 --> hate      \n",
      "april                --> april     \n",
      "fools                --> fool      \n",
      "day                  --> day       \n",
      "angry                --> angri     \n",
      "theres               --> there     \n",
      "a                    --> a         \n",
      "site                 --> site      \n",
      "im                   --> im        \n",
      "regularly            --> regular   \n",
      "on                   --> on        \n",
      "and                  --> and       \n",
      "the                  --> the       \n",
      "admins               --> admin     \n",
      "are                  --> are       \n",
      "screwing             --> screw     \n",
      "everything           --> everyth   \n",
      "up                   --> up        \n",
      "today                --> today     \n",
      "for                  --> for       \n",
      "a                    --> a         \n",
      "laugh                --> laugh     \n",
      "but                  --> but       \n",
      "i                    --> i         \n",
      "dont                 --> dont      \n",
      "find                 --> find      \n",
      "it                   --> it        \n",
      "funny                --> funni     \n",
      "im                   --> im        \n",
      "actually             --> actual    \n",
      "quite                --> quit      \n",
      "freaked              --> freak     \n",
      "out                  --> out       \n",
      "about                --> about     \n",
      "it                   --> it        \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['stem'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFln-NFtVAAI"
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "A very similar operation to stemming is called lemmatization. Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n",
    "\n",
    "Sometimes you will wind up with a very similar word, but other times you will wind up with a completely different word. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nivlmH3rVAAJ",
    "outputId": "44c3133a-81b6-4c75-cc46-f4e8ef7f387c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mgirruite.DEDAT.000\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lemmatize all of the words in the MBTI dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8ejyfxbVAAK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxUiwXfIVAAN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti['lemma'] = all_mbti['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print out the results of the lemmatization to see what we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3loMYtFVAAO",
    "outputId": "12581d00-60bf-409e-b554-02530fd535b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i                    --> i         \n",
      "hate                 --> hate      \n",
      "april                --> april     \n",
      "fools                --> fool      \n",
      "day                  --> day       \n",
      "angry                --> angry     \n",
      "theres               --> there     \n",
      "a                    --> a         \n",
      "site                 --> site      \n",
      "im                   --> im        \n",
      "regularly            --> regularly \n",
      "on                   --> on        \n",
      "and                  --> and       \n",
      "the                  --> the       \n",
      "admins               --> admins    \n",
      "are                  --> are       \n",
      "screwing             --> screwing  \n",
      "everything           --> everything\n",
      "up                   --> up        \n",
      "today                --> today     \n",
      "for                  --> for       \n",
      "a                    --> a         \n",
      "laugh                --> laugh     \n",
      "but                  --> but       \n",
      "i                    --> i         \n",
      "dont                 --> dont      \n",
      "find                 --> find      \n",
      "it                   --> it        \n",
      "funny                --> funny     \n",
      "im                   --> im        \n",
      "actually             --> actually  \n",
      "quite                --> quite     \n",
      "freaked              --> freaked   \n",
      "out                  --> out       \n",
      "about                --> about     \n",
      "it                   --> it        \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['lemma'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZomXVzoVAAR"
   },
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_7g3o3SVAAR"
   },
   "source": [
    "Stop words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return a vast amount of unnecessary information.  See this [blog post](http://xpo6.com/list-of-english-stop-words/) for more information. `nltk` has a corpus of stopwords. Let's print out the stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XY1p3uZVAAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD6ovPGLVAAW",
    "outputId": "aae2892f-0e10-4325-f3a7-3e7091352282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stopwords.words('english'))[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function removes all of the English stopwords from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPDdMJ2SVAAX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5PBwZRUVAAb"
   },
   "source": [
    "Let's leave the stop words in for now so that we can test the following **Hypothesis**:\n",
    "* Introverts tend to use the word **`I`** more than extroverts\n",
    "* Conversely, Extroverts tend to favour the word **`you`**\n",
    "\n",
    "In case you want to run the analysis again without stop words, uncomment and run the following cell! Be warned, this can take long with the pandas apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyeHeXlAVAAb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_mbti['stem'] = all_mbti['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn about text feature extraction and how these methods will help us test our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text feature extraction\n",
    "\n",
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbRb9FHYVAAg"
   },
   "source": [
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect `dict` style feature sets, so we must therefore transform our text into a `dict`. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words in the text, indicating the number of times each word has appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mg8PtprJVAAg",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a set of dictionaries, one for each of the MBTI types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtqJO_YhVAAh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "personality = {}\n",
    "for pp in type_labels:\n",
    "    df = all_mbti.groupby('type')\n",
    "    personality[pp] = {}\n",
    "    for row in df.get_group(pp)['tokens']:\n",
    "        personality[pp] = bag_of_words_count(row, personality[pp])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a list of all of the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ky-rofa_VAAi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for pp in type_labels:\n",
    "    for word in personality[pp]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done so that we can create a combined bag of words dictionary for all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpjTsIOsVAAl",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "personality['all'] = {}\n",
    "for pp in type_labels:    \n",
    "    for word in all_words:\n",
    "        if word in personality[pp].keys():\n",
    "            if word in personality['all']:\n",
    "                personality['all'][word] += personality[pp][word]\n",
    "            else:\n",
    "                personality['all'][word] = personality[pp][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily calculate how many words there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUGBaAELVAAp",
    "outputId": "beff7596-19e4-43bb-ecce-291231be4c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8206381"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = sum([v for v in personality['all'].values()])\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of words which occur less than 10 times in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiSrw4V3VAAn",
    "outputId": "b59469fe-8252-4546-94b3-76c1d9ac59bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'word frequency')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEJCAYAAABR4cpEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAt0UlEQVR4nO3dfXxT9cH//1faBBTTa1iW0F4dw3mzucEucMS5KqbitDe2sVhFkSpTx1C2ARfTaq9SqVWY6BUpopbpNef1ELeHq3etdCHoRG7rsHQPYVwC2xww7lZ6A/YGWtLk/P7gS35UhZIe0lB5Px8PHzWfc5Lz/tRu75xzknMshmEYiIiImBAX6wAiItL/qUxERMQ0lYmIiJimMhEREdNUJiIiYprKRERETFOZiIiIadZYB4iVAwfaCYV69xWbIUPsNDW1neZE5ilXZJQrMsoVmS9brrg4C+eff94Jl5+1ZRIKGb0uk2PPPxMpV2SUKzLKFZmzKZcOc4mIiGkqExERMU1lIiIipqlMRETENJWJiIiYpjIRERHTolomVVVVZGdnk52dzRNPPAFATU0NHo+H9PR0ysrKwutu2bKFvLw8MjIymD17Nl1dXQDs3buX/Px8MjMzmTZtGu3t7QC0tLQwdepUsrKyyM/Pp6GhIZpTERGRk4hamRw+fJh58+axZMkSqqqq2LBhAytWrKCoqIjy8nJ8Ph+bN29m1apVABQUFDBnzhyWL1+OYRhUVFQAUFpayqRJk/D7/YwcOZLy8nIAFi5ciMvlYtmyZUyYMIF58+ZFayqfcyQQxOFI6PN/Ev7t3D6bo4hIJKL2pcVgMEgoFOLw4cMMGjSIrq4u7HY7w4cPZ9iwYQB4PB78fj8XX3wxHR0djB49GoC8vDwWLVrEhAkTqK2t5bnnnguP33HHHRQUFLBy5Up++9vfApCTk8Ojjz5KIBDAZrNFa0phA2zxeO6vivp2PmvpU7m09vlWRUR6FrUysdvtzJw5k6ysLM4991wuv/xy9u/fj8PhCK/jdDqpr6//3LjD4aC+vp4DBw5gt9uxWq3dxoFuz7Fardjtdpqbmxk6dOgp5RsyxH66ptqnHI4EU8tjRbkio1yRUa7IRCNX1Mpk69atvPHGG7z//vskJCTwwAMPsGPHDiwWS3gdwzCwWCyEQqEvHD/283iffXz8c+LiTv2oXVNTW68vKRDLP5CGhhPvmzgcCSddHivKFRnlioxyRaa3ueLiLCd9Ex61cyZr164lNTWVIUOGMGDAAPLy8li/fn23E+UNDQ04nU6SkpK6jTc2NuJ0OklMTKS1tZVgMNhtfTi6V9PY2AhAV1cX7e3tDB48OFrTERGRk4hamVx66aXU1NRw6NAhDMNgxYoVjBo1iu3bt7Nz506CwSDV1dW43W5SUlIYOHAgdXV1wNFPgbndbmw2Gy6XC5/PB0BlZSVutxuAtLQ0KisrAfD5fLhcrj45XyIiIp8XtcNcY8eO5eOPPyYvLw+bzcZ3v/tdpk+fzlVXXcX06dPp7OwkLS2NzMxMALxeL8XFxbS1tTFixAgmT54MQElJCYWFhSxevJjk5GQWLFgAwMyZMyksLCQ7O5uEhAS8Xm+0piIiIj2wGIZxZl4jOcrMnjOJ1ae5dM7k9FGuyChXZL5suWJ2zkRERM4eKhMRETFNZSIiIqapTERExDSViYiImKYyERER01QmIiJimspERERMU5mIiIhpKhMRETFNZSIiIqapTERExDSViYiImKYyERER01QmIiJimspERERMi9qdFl977TVeeeWV8OPdu3eTm5vLddddx+OPP05nZydZWVnMmjULgC1btjB79mza29txuVyUlpZitVrZu3cvBQUFNDU18Y1vfAOv18t5551HS0sLDzzwALt27SIxMZGFCxficDiiNR0RETmJqO2ZTJgwgaqqKqqqqvB6vQwZMoSf/OQnFBUVUV5ejs/nY/PmzaxatQqAgoIC5syZw/LlyzEMg4qKCgBKS0uZNGkSfr+fkSNHUl5eDsDChQtxuVwsW7aMCRMmMG/evGhNRUREetAnh7keeeQRZs2axa5duxg+fDjDhg3DarXi8Xjw+/3s2bOHjo4ORo8eDUBeXh5+v59AIEBtbS0ZGRndxgFWrlyJx+MBICcnh9WrVxMIBPpiOiIi8hlRL5Oamho6OjrIyspi//793Q5FOZ1O6uvrPzfucDior6/nwIED2O12rFZrt3Gg23OsVit2u53m5uZoT0dERL5A1M6ZHPPqq69y9913AxAKhbBYLOFlhmFgsVhOOH7s5/E++/j458TFnXo3Dhlij2QaZwyHI8HU8lhRrsgoV2SUKzLRyBXVMjly5Ai1tbXMnz8fgKSkJBoaGsLLGxoacDqdnxtvbGzE6XSSmJhIa2srwWCQ+Pj48PpwdK+msbGRpKQkurq6aG9vZ/DgwaecrampjVDI6NW8YvkH0tDQesJlDkfCSZfHinJFRrkio1yR6W2uuDjLSd+ER/Uw17Zt27jgggsYNGgQAKNGjWL79u3s3LmTYDBIdXU1breblJQUBg4cSF1dHQBVVVW43W5sNhsulwufzwdAZWUlbrcbgLS0NCorKwHw+Xy4XC5sNls0pyMiIicQ1T2TXbt2kZSUFH48cOBA5s+fz/Tp0+ns7CQtLY3MzEwAvF4vxcXFtLW1MWLECCZPngxASUkJhYWFLF68mOTkZBYsWADAzJkzKSwsJDs7m4SEBLxebzSnIiIiJ2ExDKN3x3r6ObOHuTz3V53mRD1b+lSuDnOdRsoVGeWKzJctV0wPc4mIyNlBZSIiIqapTERExDSViYiImKYyERER01QmIiJimspERERMU5mIiIhpKhMRETFNZSIiIqapTERExDSViYiImKYyERER01QmIiJimspERERMU5mIiIhpKhMRETEtqmWyYsUK8vLyyMrKYu7cuQDU1NTg8XhIT0+nrKwsvO6WLVvIy8sjIyOD2bNn09XVBcDevXvJz88nMzOTadOm0d7eDkBLSwtTp04lKyuL/Px8GhoaojkVERE5iaiVya5duygpKaG8vJy3336bjz/+mFWrVlFUVER5eTk+n4/NmzezatUqAAoKCpgzZw7Lly/HMAwqKioAKC0tZdKkSfj9fkaOHEl5eTkACxcuxOVysWzZMiZMmMC8efOiNRUREelB1Mrk3Xff5YYbbiApKQmbzUZZWRnnnnsuw4cPZ9iwYVitVjweD36/nz179tDR0cHo0aMByMvLw+/3EwgEqK2tJSMjo9s4wMqVK/F4PADk5OSwevVqAoFAtKYjIiInYY3WC+/cuRObzcZ9993Hvn37uOaaa7jkkktwOBzhdZxOJ/X19ezfv7/buMPhoL6+ngMHDmC327Fard3GgW7PsVqt2O12mpubGTp0aLSmJCIiJxC1MgkGg2zYsIElS5YwaNAgpk2bxjnnnIPFYgmvYxgGFouFUCj0hePHfh7vs4+Pf05c3KnvaA0ZYo9wRmcGhyPB1PJYUa7IKFdklCsy0cgVtTL56le/SmpqKomJiQBcd911+P1+4uPjw+s0NDTgdDpJSkrqdgK9sbERp9NJYmIira2tBINB4uPjw+vD0b2axsZGkpKS6Orqor29ncGDB59yvqamNkIho1dzi+UfSEND6wmXORwJJ10eK8oVGeWKjHJFpre54uIsJ30THrVzJuPGjWPt2rW0tLQQDAZZs2YNmZmZbN++nZ07dxIMBqmursbtdpOSksLAgQOpq6sDoKqqCrfbjc1mw+Vy4fP5AKisrMTtdgOQlpZGZWUlAD6fD5fLhc1mi9Z0RETkJKK2ZzJq1CimTJnCpEmTCAQCXHXVVdx+++1ceOGFTJ8+nc7OTtLS0sjMzATA6/VSXFxMW1sbI0aMYPLkyQCUlJRQWFjI4sWLSU5OZsGCBQDMnDmTwsJCsrOzSUhIwOv1RmsqIiLSA4thGL071tPPmT3M5bm/6jQn6tnSp3J1mOs0Uq7IKFdkvmy5YnaYS0REzh4qExERMU1lIiIipqlMRETENJWJiIiYpjIRERHTVCYiImKaykRERExTmYiIiGkqExERMU1lIiIipqlMRETENJWJiIiYpjIRERHTeiyTxsZG3nvvPQD++7//mx/96Eds3bo16sFERKT/6LFMCgsL2bVrFx988AFr1qwhNzeXuXPn9kU2ERHpJ3osk4MHD3LXXXexevVqcnJyyMvL4/Dhw32RTURE+okeyyQQCBAIBFizZg1XXnklhw8f5tChQ6f04nfeeSfZ2dnk5uaSm5vLxo0bqampwePxkJ6eTllZWXjdLVu2kJeXR0ZGBrNnz6arqwuAvXv3kp+fT2ZmJtOmTaO9vR2AlpYWpk6dSlZWFvn5+TQ0NPRm/iIichr0WCY//OEPSU1N5fzzz2fkyJFMmDCBnJycHl/YMAx27NhBVVVV+J9vfetbFBUVUV5ejs/nY/PmzaxatQqAgoIC5syZw/LlyzEMg4qKCgBKS0uZNGkSfr+fkSNHUl5eDsDChQtxuVwsW7aMCRMmMG/ePDO/BxERMaHHMpkxYwbV1dW8/PLLAHi9Xn72s5/1+ML/+Mc/ALjnnnu48cYbeeWVV9i0aRPDhw9n2LBhWK1WPB4Pfr+fPXv20NHRwejRowHIy8vD7/cTCASora0lIyOj2zjAypUr8Xg8AOTk5LB69WoCgUDkvwERETHNeqIFlZWVJ3zS1q1bGT9+/ElfuKWlhdTUVB5++GECgQCTJ09mypQpOByO8DpOp5P6+nr279/fbdzhcFBfX8+BAwew2+1YrdZu40C351itVux2O83NzQwdOrTHSQMMGWI/pfXONA5HgqnlsaJckVGuyChXZKKR64RlcmwPoKGhgX/84x/84Ac/wGq1sn79er797W/3WCaXXXYZl112WfjxLbfcwqJFixgzZkx4zDAMLBYLoVAIi8XyufFjP4/32cfHPycu7tS/NtPU1EYoZJzy+seL5R9IQ0PrCZc5HAknXR4ryhUZ5YqMckWmt7ni4iwnfRN+wjL51a9+BcDUqVMpKyvj61//OnD0hPjDDz/c44Y3bNhAIBAgNTUVOPp/9ikpKd1OlDc0NOB0OklKSuo23tjYiNPpJDExkdbWVoLBIPHx8eH14eheTWNjI0lJSXR1ddHe3s7gwYN7zCUiIqdfj2/l9+3bFy4SgH//93/nX//6V48v3NraypNPPklnZydtbW289dZb/OIXv2D79u3s3LmTYDBIdXU1breblJQUBg4cSF1dHQBVVVW43W5sNhsulwufzwccPfTmdrsBSEtLCx+K8/l8uFwubDZbxL8AEREx74R7Jsc4HA4WLVrETTfdBMDvf/97hg0b1uMLjxs3jo0bNzJ+/HhCoRCTJk3isssuY/78+UyfPp3Ozk7S0tLIzMwEjp7YLy4upq2tjREjRjB58mQASkpKKCwsZPHixSQnJ7NgwQIAZs6cSWFhIdnZ2SQkJOD1env9SxAREXMshmGc9MTB/v37efTRR6mpqSEuLo6rr76ahx9+mMTExL7KGBVmz5l47q86zYl6tvSpXJ0zOY2UKzLKFZkvW65enzM5ZsmSJTz77LMRb1hERM4ePZ4zWblyZR/EEBGR/qzHPZOvfe1r3HPPPXzve9/jvPPOC4/ffffdUQ0mIiL9R49lcuzjtnv27Il2FhER6ad6LJPHH38cOFomXV1dDB8+POqhRESkf+mxTHbu3MlPf/pT9u/fTygU4vzzz+f555/noosu6ot8IiLSD/R4Av7RRx9lypQp1NbWUldXx7Rp0ygtLe2LbCIi0k/0WCZNTU3hLywC3HzzzRw4cCCqoUREpH/psUyCwSAHDx4MP25ubo5mHhER6Yd6PGdyxx13cNttt5GVlYXFYsHn8/GjH/2oL7KJiEg/0WOZ3HbbbQwfPpw1a9YQCoUoKSnhyiuv7ItsIiLST/RYJrfeeisZGRlMnDjxlC7wKCIiZ58ez5k89NBDNDc3M3XqVPLy8nj++efDt+QVERGBUyiTMWPGUFBQwLJly5gyZQqvvvoq2dnZfZFNRET6iR4Pc7311lusW7eO9evXk5KSwk033cTYsWP7IpuIiPQTPZbJ3LlzGTRoEPfeey8ZGRk4HI6+yCUiIv1Ij4e51q9fj9fr5V//+hf33nsv48eP54knnjjlDTzxxBMUFhYCUFNTg8fjIT09nbKysvA6W7ZsIS8vj4yMDGbPnk1XVxdw9H7z+fn5ZGZmMm3aNNrb2wFoaWlh6tSpZGVlkZ+f3+3+8SIi0vd6LBOr1coVV1xBRkYG1157LR0dHaxateqUXvyDDz7grbfeAqCjo4OioiLKy8vx+Xxs3rw5/DoFBQXMmTOH5cuXYxgGFRUVAJSWljJp0iT8fj8jR46kvLwcgIULF+JyuVi2bBkTJkxg3rx5vZq8iIicHj2WyYMPPshVV13FI488gtVq5bnnnsPn8/X4wgcPHqSsrIz77rsPgE2bNjF8+HCGDRuG1WrF4/Hg9/vZs2cPHR0djB49GoC8vDz8fj+BQIDa2loyMjK6jcPRG3Z5PB4AcnJyWL16NYFAoFe/ABERMa/HcyYjRoxg1qxZJCcnR/TCc+bMYdasWezbtw84ei/548+3OJ1O6uvrPzfucDior6/nwIED2O12rFZrt/HPvpbVasVut9Pc3MzQoUMjyigiIqdHj2XSm0unvPbaayQnJ5Oamsqbb74JQCgUwmKxhNcxDAOLxXLC8WM/j/fZx8c/Jy6ux52sboYMsUe0/pnC4UgwtTxWlCsyyhUZ5YpMNHL1WCa94fP5aGhoIDc3l08//ZRDhw6xZ88e4uPjw+s0NDTgdDpJSkrqdgK9sbERp9NJYmIira2tBINB4uPjw+vD0b2axsZGkpKS6Orqor29PXxHyFPV1NRGKGT0an6x/ANpaGg94TKHI+Gky2NFuSKjXJFRrsj0NldcnOWkb8JP+HZ+48aNEW/smJdeeonq6mqqqqqYMWMG1157Lb/+9a/Zvn07O3fuJBgMUl1djdvtJiUlhYEDB1JXVwdAVVUVbrcbm82Gy+UKn5+prKzE7XYDkJaWRmVlJXC0uFwuFzabrdd5RUTEnBOWySOPPAL07jDXFxk4cCDz589n+vTp3HDDDVx44YVkZmYC4PV6efzxx8nMzOTQoUNMnjwZgJKSEioqKrjhhhvYsGED//mf/wnAzJkz+eijj8jOzuZ3v/sdc+bMOS0ZRUSkdyyGYXzhsR6Px4PD4eAvf/kLY8aM+dzyX/3qV1EPF01mD3N57q86zYl6tvSpXB3mOo2UKzLKFZkvW66eDnOd8JzJ//zP//CnP/2J7du3hz+eKyIi8kVOWCZJSUmMHz+e5ORkrrjiCvbs2UNXVxfDhw/vy3wiItIP9PhprqFDh5Kdnc3+/fsJhUKcf/75PP/881x00UV9kU9ERPqBHr+c8dhjjzFlyhRqa2upq6tj2rRplJaW9kU2ERHpJ3osk6amJm666abw45tvvpkDBw5ENZSIiPQvPZZJMBjk4MGD4cfNzc3RzCMiIv1Qj+dM7rjjDm677TaysrKwWCz4fL7T9t0TERH5cuixTG677Ta+/vWvs3btWkKhECUlJVx55ZV9kU1ERPqJU7o2V2pqKqmpqdHOIiIi/VRkl9oVERH5AioTERExrccy+d3vfveF/y4iInLMCcskMzOThx56iJdeeomtW7cSCAR47bXX+jKbiIj0Eycsk7fffpubb76ZtrY2nnvuOTweDzt27GDevHm8++67fZlRRETOcCcsk7179/L973+foUOH8swzz+D3+/na177GFVdcwZ///Oe+zCgiIme4E340+LHHHmP37t20tLTwwgsv8J3vfAeA6667juuuu67PAoqIyJnvhHsmL774In/4wx8477zzSEhI4N1332XXrl3k5OTozoYiItLNSb+0aLVaufDCC7n99tsB2LdvHwsXLuSjjz46pRd/+umnWb58ORaLhVtuuYW7776bmpoaHn/8cTo7O8nKymLWrFkAbNmyhdmzZ9Pe3o7L5aK0tBSr1crevXspKCigqamJb3zjG3i9Xs477zxaWlp44IEH2LVrF4mJiSxcuBCHw2HutyEiIr3S40eDX3jhhW7/PmjQoFO6nMqHH37In/70J95++23eeOMNlixZwtatWykqKqK8vByfz8fmzZtZtWoVAAUFBcyZM4fly5djGAYVFRUAlJaWMmnSJPx+PyNHjqS8vByAhQsX4nK5WLZsGRMmTGDevHm9+gWIiIh5UfvS4ve//31efvllrFYrTU1NBINBWlpaGD58OMOGDcNqteLxePD7/ezZs4eOjg5Gjx4NQF5eHn6/n0AgQG1tbfi2wcfGAVauXInH4wEgJyeH1atXEwgEojUdERE5iVO6Nldv2Ww2Fi1axG9+8xsyMzPZv39/t0NRTqeT+vr6z407HA7q6+s5cOAAdrsdq9XabRzo9hyr1Yrdbqe5uZmhQ4eeUrYhQ+yna5p9yuFIMLU8VpQrMsoVGeWKTDRyRbVMAGbMmMFPfvIT7rvvPnbs2IHFYgkvMwwDi8VCKBT6wvFjP4/32cfHPycu7tR3tJqa2giFjAhnc1Qs/0AaGlpPuMzhSDjp8lhRrsgoV2SUKzK9zRUXZznpm/CoHeb65JNP2LJlCwDnnnsu6enprF+/noaGhvA6DQ0NOJ1OkpKSuo03NjbidDpJTEyktbWVYDDYbX04ulfT2NgIQFdXF+3t7QwePDha0xERkZOIWpns3r2b4uJijhw5wpEjR3jvvfeYOHEi27dvZ+fOnQSDQaqrq3G73aSkpDBw4EDq6uoAqKqqwu12Y7PZcLlc+Hw+ACorK3G73QCkpaVRWVkJgM/nw+VyYbPZojUdERE5iagd5kpLS2PTpk2MHz+e+Ph40tPTyc7OJjExkenTp9PZ2UlaWhqZmZkAeL1eiouLaWtrY8SIEUyePBmAkpISCgsLWbx4McnJySxYsACAmTNnUlhYSHZ2NgkJCXi93mhNRUREemAxDKN3Jw76ObPnTDz3V53mRD1b+lSuzpmcRsoVGeWKzJctV8zOmYiIyNlDZSIiIqapTERExDSViYiImKYyERER01QmIiJimspERERMU5mIiIhpKhMRETFNZSIiIqapTERExDSViYiImKYyERER01QmIiJimspERERMU5mIiIhpUS2TZ599luzsbLKzs3nyyScBqKmpwePxkJ6eTllZWXjdLVu2kJeXR0ZGBrNnz6arqwuAvXv3kp+fT2ZmJtOmTaO9vR2AlpYWpk6dSlZWFvn5+d3uIS8iIn0ramVSU1PD2rVreeutt6isrOT//u//qK6upqioiPLycnw+H5s3b2bVqlUAFBQUMGfOHJYvX45hGFRUVABQWlrKpEmT8Pv9jBw5kvLycgAWLlyIy+Vi2bJlTJgwgXnz5kVrKiIi0oOolYnD4aCwsJABAwZgs9m46KKL2LFjB8OHD2fYsGFYrVY8Hg9+v589e/bQ0dHB6NGjAcjLy8Pv9xMIBKitrSUjI6PbOMDKlSvxeDwA5OTksHr1agKBQLSmIyIiJxG1MrnkkkvC5bBjxw6WLVuGxWLB4XCE13E6ndTX17N///5u4w6Hg/r6eg4cOIDdbsdqtXYbB7o9x2q1YrfbaW5ujtZ0RETkJKzR3sDf/vY37r33Xh588EHi4+PZsWNHeJlhGFgsFkKhEBaL5XPjx34e77OPj39OXNypd+OQIfbIJnKGcDgSTC2PFeWKjHJFRrkiE41cUS2Turo6ZsyYQVFREdnZ2Xz44YfdTpQ3NDTgdDpJSkrqNt7Y2IjT6SQxMZHW1laCwSDx8fHh9eHoXk1jYyNJSUl0dXXR3t7O4MGDTzlbU1MboZDRq3nF8g+koaH1hMscjoSTLo8V5YqMckVGuSLT21xxcZaTvgmP2mGuffv28bOf/Qyv10t2djYAo0aNYvv27ezcuZNgMEh1dTVut5uUlBQGDhxIXV0dAFVVVbjdbmw2Gy6XC5/PB0BlZSVutxuAtLQ0KisrAfD5fLhcLmw2W7SmIyIiJxG1PZMXX3yRzs5O5s+fHx6bOHEi8+fPZ/r06XR2dpKWlkZmZiYAXq+X4uJi2traGDFiBJMnTwagpKSEwsJCFi9eTHJyMgsWLABg5syZFBYWkp2dTUJCAl6vN1pTERGRHlgMw+jdsZ5+zuxhLs/9Vac5Uc+WPpWrw1ynkXJFRrki82XLFbPDXCIicvZQmYiIiGkqExERMU1lIiIipqlMRETENJWJiIiYpjIRERHTVCYiImKaykRERExTmYiIiGkqExERMS3q9zOR0+dIIBiT+5l0dHbR2nL4tL+uiHx5qEz6kQG2+JhdYPLMu1ydiJxJdJhLRERMU5mIiIhpKhMRETFNZSIiIqZFtUza2trIyclh9+7dANTU1ODxeEhPT6esrCy83pYtW8jLyyMjI4PZs2fT1dUFwN69e8nPzyczM5Np06bR3t4OQEtLC1OnTiUrK4v8/HwaGhqiOQ0REelB1Mpk48aN3H777ezYsQOAjo4OioqKKC8vx+fzsXnzZlatWgVAQUEBc+bMYfny5RiGQUVFBQClpaVMmjQJv9/PyJEjKS8vB2DhwoW4XC6WLVvGhAkTmDdvXrSmISIipyBqZVJRUUFJSQlOpxOATZs2MXz4cIYNG4bVasXj8eD3+9mzZw8dHR2MHj0agLy8PPx+P4FAgNraWjIyMrqNA6xcuRKPxwNATk4Oq1evJhAIRGsqIiLSg6h9z+Szewv79+/H4XCEHzudTurr6z837nA4qK+v58CBA9jtdqxWa7fxz76W1WrFbrfT3NzM0KFDozUdERE5iT770mIoFMJisYQfG4aBxWI54fixn8f77OPjnxMXF9lO1pAh9ojWP9uZ/WZ9NL6ZfzooV2SUKzJnU64+K5OkpKRuJ8obGhpwOp2fG29sbMTpdJKYmEhrayvBYJD4+Pjw+nB0r6axsZGkpCS6urpob29n8ODBEeVpamojFDJ6NZcz9Q8kmhoaev8deIcjwdTzo0W5IqNckfmy5YqLs5z0TXiffTR41KhRbN++nZ07dxIMBqmursbtdpOSksLAgQOpq6sDoKqqCrfbjc1mw+Vy4fP5AKisrMTtdgOQlpZGZWUlAD6fD5fLhc1m66upiIjIZ/TZnsnAgQOZP38+06dPp7Ozk7S0NDIzMwHwer0UFxfT1tbGiBEjmDx5MgAlJSUUFhayePFikpOTWbBgAQAzZ86ksLCQ7OxsEhIS8Hq9fTUNERH5AlEvkxUrVoT/PTU1lbfffvtz61x66aW8/vrrnxtPSUlhyZIlnxsfPHgwv/rVr05vUBER6TV9A15ERExTmYiIiGkqExERMU1lIiIipulOi9KjU7ldcE96+3zdMlikf1CZSI9idbtg0C2DRfoLHeYSERHTVCYiImKaykRERExTmYiIiGk6AS9ntNPxSbKTOdFr61NkIpFRmcgZLVafJNOnyEQio8NcIiJimvZMRM4w0T60dyI6tCdmqExEzjA6tCf9kcpERIBT2yOK1h6T9or6P5WJiACxvWzOG/NzTBWVrv0We/26TJYuXcrixYvp6uriRz/6Efn5+bGOJCK9EKsiM1tiPTmbPnreb8ukvr6esrIy3nzzTQYMGMDEiRO54ooruPjii2MdTUT6iS9riZ3MkUAwKq/bb8ukpqaGH/zgBwwePBiAjIwM/H4/P//5z0/p+XFxFlPbd55/rqnna7tn/rbN/o2Yof/OX+7tDrDF8+O57/T5dgFeLE7v1d92T8+xGIZh9DZULD3//PMcOnSIWbNmAfDaa6+xadMmHnvssRgnExE5+/TbLy2GQiEslv+/KQ3D6PZYRET6Tr8tk6SkJBoaGsKPGxoacDqdMUwkInL26rdlcuWVV/LBBx/Q3NzM4cOHeeedd3C73bGOJSJyVuq3J+CHDh3KrFmzmDx5MoFAgFtuuYX/+I//iHUsEZGzUr89AS8iImeOfnuYS0REzhwqExERMU1lIiIipqlMRETENJVJhNra2sjJyWH37t2xjhL27LPPkp2dTXZ2Nk8++WSs44Q9/fTT3HDDDWRnZ/PSSy/FOs7nPPHEExQWFsY6Rtidd95JdnY2ubm55ObmsnHjxlhHAmDFihXk5eWRlZXF3LlzYx0HOHrFi2O/p9zcXMaMGcOjjz4a61gAVFVVhf/3+MQTT8Q6TtgLL7xARkYGHo+HxYsXn/4NGHLKPvroIyMnJ8cYMWKEsWvXrljHMQzDMNatW2fcdtttRmdnp3HkyBFj8uTJxjvvvBPrWMb69euNiRMnGoFAwDh8+LAxbtw445NPPol1rLCamhrjiiuuMB566KFYRzEMwzBCoZAxduxYIxAIxDpKN//85z+NsWPHGvv27TOOHDli3H777cbKlStjHaubv/71r8b1119vNDU1xTqKcejQIePyyy83mpqajEAgYNxyyy3GunXrYh3LWLdunZGTk2O0trYaXV1dxr333mssX778tG5DeyYRqKiooKSk5Iz6pr3D4aCwsJABAwZgs9m46KKL2Lt3b6xj8f3vf5+XX34Zq9VKU1MTwWCQQYMGxToWAAcPHqSsrIz77rsv1lHC/vGPfwBwzz33cOONN/LKK6/EONFR7777LjfccANJSUnYbDbKysoYNWpUrGN188gjjzBr1iwSExNjHYVgMEgoFOLw4cN0dXXR1dXFwIEDYx2Ljz/+mLFjx2K324mPj+fqq6/mj3/842ndhsokAvPmzcPlcsU6RjeXXHIJo0ePBmDHjh0sW7aMtLS02Ib6f2w2G4sWLSI7O5vU1FSGDh0a60gAzJkzh1mzZvFv//ZvsY4S1tLSQmpqKs899xz/+7//y6uvvsq6detiHYudO3cSDAa57777yM3N5Xe/+x1f+cpXYh0rrKamho6ODrKysmIdBQC73c7MmTPJysoiLS2NlJQUvve978U6FiNGjGDt2rUcPHiQzs5OVqxYQWNj42ndhsrkS+Jvf/sb99xzDw8++CAXXHBBrOOEzZgxgw8++IB9+/ZRUVER6zi89tprJCcnk5qaGuso3Vx22WU8+eSTJCQkkJiYyC233MKqVatiHYtgMMgHH3zAL3/5S37/+9+zadMm3nrrrVjHCnv11Ve5++67Yx0jbOvWrbzxxhu8//77rFmzhri4OF588cVYxyI1NZW8vDzuvPNOpkyZwpgxY7DZbKd1GyqTL4G6ujruuusu7r//fm666aZYxwHgk08+YcuWLQCce+65pKens23bthinAp/Px7p168jNzWXRokWsWLGCX/7yl7GOxYYNG/jggw/Cjw3DwGqN/dWOvvrVr5KamkpiYiLnnHMO1113HZs2bYp1LACOHDlCbW0t1157bayjhK1du5bU1FSGDBnCgAEDyMvL48MPP4x1LNra2khPT2fp0qUsWbKEAQMGMGzYsNO6DZVJP7dv3z5+9rOf4fV6yc7OjnWcsN27d1NcXMyRI0c4cuQI7733HmPGjIl1LF566SWqq6upqqpixowZXHvttRQVFcU6Fq2trTz55JN0dnbS1tbGW2+9xfXXXx/rWIwbN461a9fS0tJCMBhkzZo1jBgxItaxANi2bRsXXHDBGXMuDuDSSy+lpqaGQ4cOYRgGK1as4Lvf/W6sY7F7925++tOf0tXVRWtrK6+//vppPzQY+7c+YsqLL75IZ2cn8+fPD49NnDiR22+/PYapIC0tjU2bNjF+/Hji4+NJT08/o8ruTDNu3Dg2btzI+PHjCYVCTJo0icsuuyzWsRg1ahRTpkxh0qRJBAIBrrrqKm6++eZYxwJg165dJCUlxTpGN2PHjuXjjz8mLy8Pm83Gd7/7XaZOnRrrWFx66aWkp6dz4403EgwGueuuu077mztd6FFEREzTYS4RETFNZSIiIqapTERExDSViYiImKYyERER01QmIqdJc3Mz3/rWt75w2Ztvvsk111zDj3/84z5OJdI39D0TkT5QWVnJrFmzyM3NjXUUkahQmchZJTc3l8LCQlJTU6murua//uu/qK2t5ZxzzmH27NmMGDECj8dDaWkpW7duxWKxcPXVV/OLX/wCq9XKyJEj+eEPf8jWrVvxer3s27ePsrIyzj33XEaOHPmF2/zlL3/JX/7yF3bv3s2BAwfYunUrBw8eZNeuXVxzzTXMnDkTr9dLbW0twWCQ73znOxQXF2O326mtrWXu3LlYLBZGjRrFmjVrePnll9mzZw+PPfYY1dXVAKxfv77b48WLF/POO+8QCoVISUmhpKSEoUOHcueddzJ69Gj+/Oc/s2/fPlJTU3nssceIi4vj/fffZ+HChYRCIQYNGkRpaSnvv/8+f//733nqqaeAo5d9mTt3LpWVlX3y30v6Dx3mkrPK9ddfz+rVqwFYs2YNX/nKV9iwYQOGYbBq1Squv/565s6dy+DBg1m6dClvvPEG27Zt4ze/+Q0AgUCAcePGsXz5cpKTkykqKuKZZ57hzTffJCUl5Qu3WVRUxMiRI3nwwQe56667AOjo6OAPf/gDBQUFvPDCC8THx/Pmm2/y9ttv43Q68Xq9HDlyhBkzZvDQQw9RWVnJmDFj2LNnT49zrKys5K9//SuvvfYaVVVVpKWlUVxcHF7+z3/+kyVLlvD222+zevVqPvzwQxobGykoKODxxx9n6dKl/PjHP8br9XLrrbeycuVKDh48CBy9DcPEiRNN/BeQLyuViZxVjpWJYRhs2LCBu+66i3Xr1vHRRx/x9a9/HYfDwerVq7njjjuwWCwMGDCAiRMnhgsICN+GoK6ujm9+85tcfPHFANx2222nnOP4S1msXLmSFStWMH78eHJzc/njH//IJ598wrZt2xgwYABXXnklADfeeOMpXTb//fffZ+PGjdx8883k5ubyyiuvsH379vDycePGERcXh91uZ/jw4Xz66af8+c9/5pJLLuE73/kOAOnp6fz6179myJAhXHPNNVRVVfHpp5+ydu1aPB7PKc9Tzh46zCVnlW9961sEAgHee+89LrjgAsaNG8esWbOwWq1kZGQAEAqFsFgs4eeEQiG6urrCj4+/sODxVyOK5Cq/x79GKBSiqKgofB+a9vZ2Ojs7aWpq4rNXOzq2DYvF0m1ZIBDo9nrHrqcFR6+u++mnn4aXn3POOeF/P/Y6Vqu125wNw2Dbtm1ceuml5Ofn88gjj2C1WklPT+e888475XnK2UN7JnLWue6663jqqae46qqruOiii2hra2Pp0qWkp6cDRy/W98orr2AYBkeOHKGioiK8d3C8yy+/nL///e9s3boVOPqJrd4YO3Ysv/3tbzly5AihUIiHH36YBQsW8I1vfINzzjmHFStWALBq1Sqam5sBSExMZO/eveHC+cMf/tDt9V5//XXa2toAePrpp3nwwQdPmmHUqFF88skn/O1vfwPgvffeo6CgAIDvfe974fty6BCXnIj2TOSsc/311/Piiy+GC+LKK69k27ZtJCcnA1BcXMzcuXPxeDwEAgGuvvrqL7zFb2JiIl6vlwceeACbzcbll1/eqzw//elPeeKJJ7jpppsIBoN8+9vfprCwEKvVyjPPPENpaSmLFi3im9/8ZvgWsBdffDETJ07k5ptvxuFwcM011/CXv/wFgAkTJlBfX8+tt96KxWIhOTm521Wlv8hXv/pVvF4vDz30EMFgELvdTllZWXh5Xl4ePp+PSy+9tFdzlC8/XTVYpB+57LLLWLp0KV/72tf6bJtdXV38/Oc/58Ybb+SGG27os+1K/6LDXCJyQn//+99JTU3l/PPPJzMzM9Zx5AymPRMRETFNeyYiImKaykRERExTmYiIiGkqExERMU1lIiIipqlMRETEtP8PA+pE7h75s3EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist([v for v in personality['all'].values() if v < 10],bins=10)\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QspSnpFzVAAp"
   },
   "source": [
    "There are a lot of words that only appear once! We'll print out that value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBsFSL29VAAw",
    "outputId": "64e625a1-15be-4541-b802-632e803b903a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80842"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([v for v in personality['all'].values() if v == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of words do you think would appear once out of roughly 8 million words? Let's print out a few of these rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['navelunderactive25', 'typewinginstinct', 'braus', 'superseding', 'jovovichs', 'a4w3', 'selfexpress', 'reasonsbesidesthere', 'truthat', 'loooveee', 'religionthey', 'bargeld', 'wikepedia', 'babyshambles', 'listeningbut', 'chugathon', 'nowhaha', 'authorssee', 'theseill', 'careersdue', 'choosethere', 'butterfingers', 'shaddie', 'ohithat', 'oryx', 'urlwebwatchvfafjxk2nvtwampdesktopuri2fwatch3fv3dfaf', 'lithgow', 'outearn', 'patternto', 'cute0', 'demisexualism', '9780140126785', 'butyour', 'regrouping', 'wren', 'rejectednot', 'nzqa', 'windmills', 'architectureliterature', 'hypatia', 'cardassian', 'firty', 'onusedlied', '6897268973689746897568977', 'storiesbut', 'guywho', '19656', 'trubrite', 'sufficently', 'floatation', 'tbwtas', 'infppeople', 'apartmentand', 'ehalthy', 'autobrain', 'asjdfoiasod', 'amuricah', 'gocause', 'manymost', 'luckthing', 'pcill', 'customdesigned', 'nonclassical', 'latewinterspring', 'ecchi', 'shareyou', 'ninetales', 'butwant', 'workn', 'guvment', 'stfriends', 'brukar', 'fjinfj', 'threadsry', 'schuhmacher', 'sandwedge', 'chodleros', 'facesfeatures', 'allwink', 'dhowever', 'copiers', 'halfpaying', 'enneagramwhen', 'goingtongue', 'sandburg', 'bedbut', 'mincing', 'hummmi', 'journaled', 'hideaway', 'viewp', 'possesive', 'eyescome', 'lavendar', 'oligarchic', '4828', 'wayinterestingunless', '570642570650', 'beliefvalue', 'highestflying']\n"
     ]
    }
   ],
   "source": [
    "rare_words = [k for k, v in personality['all'].items() if v==1] \n",
    "print(rare_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of these words don't make sense, but before we decide to remove them, let's see how much data we'll be left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amu9ABO8VAAz",
    "outputId": "3d99bd9f-f5eb-4c26-f3e4-d1751eee8920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18180\n",
      "8002188\n"
     ]
    }
   ],
   "source": [
    "# how many words appear more than 10 times?\n",
    "# how many words of the total does that account for?\n",
    "print(len([v for v in personality['all'].values() if v >= 10]))\n",
    "occurs_more_than_10_times = sum([v for v in personality['all'].values() if v >= 10])\n",
    "print(occurs_more_than_10_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwmyzJewVAA2",
    "outputId": "7e64933d-1034-4c8f-d160-36472eeb154f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975117777251629"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurs_more_than_10_times/total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxW-TeheVAA4"
   },
   "source": [
    "Using words that appear more than 10 times seems much more useful!  And this accounts for 97% of all the words!\n",
    "\n",
    "Finally, let's remove all words that occur less than 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZNdn2n6VAA4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_count = 10\n",
    "remaining_word_index = [k for k, v in personality['all'].items() if v > max_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "Remember our Hypothesis from earlier?:\n",
    "\n",
    "- Introverts tend to use the word `I` more than extroverts\n",
    "- Conversely, Extroverts tend to favour the word `you`\n",
    "\n",
    "Let's see if we finally have what we need to test it out. We'll first create one big dataframe with the word counts by personality profile (this may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW5qiwvrVAA5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hm = []\n",
    "for p, p_bow in personality.items():\n",
    "    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in remaining_word_index], columns=['Word', p])\n",
    "    df_bow.set_index('Word', inplace=True)\n",
    "    hm.append(df_bow)\n",
    "\n",
    "# create one big dataframe\n",
    "df_bow = pd.concat(hm, axis=1)\n",
    "df_bow.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 words which appear most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaKScx05VAA7",
    "outputId": "5bcaa326-858b-478f-9f88-f582ce599a20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>67871.0</td>\n",
       "      <td>27403.0</td>\n",
       "      <td>52115.0</td>\n",
       "      <td>43864.0</td>\n",
       "      <td>8881.0</td>\n",
       "      <td>8687.0</td>\n",
       "      <td>87712.0</td>\n",
       "      <td>31198.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>8172.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>378402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>39663.0</td>\n",
       "      <td>18994.0</td>\n",
       "      <td>35868.0</td>\n",
       "      <td>30500.0</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>5019.0</td>\n",
       "      <td>48008.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>5143.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>230247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>40231.0</td>\n",
       "      <td>17852.0</td>\n",
       "      <td>33005.0</td>\n",
       "      <td>28753.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>5106.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>227371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>31932.0</td>\n",
       "      <td>14728.0</td>\n",
       "      <td>26693.0</td>\n",
       "      <td>22780.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>40376.0</td>\n",
       "      <td>13847.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>182876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24881.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40710.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>24312.0</td>\n",
       "      <td>11335.0</td>\n",
       "      <td>21372.0</td>\n",
       "      <td>17857.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>29576.0</td>\n",
       "      <td>10217.0</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>4962.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>2976.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>138561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>22221.0</td>\n",
       "      <td>10882.0</td>\n",
       "      <td>17197.0</td>\n",
       "      <td>16010.0</td>\n",
       "      <td>3815.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>24971.0</td>\n",
       "      <td>10329.0</td>\n",
       "      <td>3332.0</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>2734.0</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>124762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>19445.0</td>\n",
       "      <td>8949.0</td>\n",
       "      <td>16396.0</td>\n",
       "      <td>14436.0</td>\n",
       "      <td>2907.0</td>\n",
       "      <td>2614.0</td>\n",
       "      <td>23445.0</td>\n",
       "      <td>8591.0</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>2207.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>110758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>18376.0</td>\n",
       "      <td>8265.0</td>\n",
       "      <td>15708.0</td>\n",
       "      <td>13179.0</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>22537.0</td>\n",
       "      <td>8028.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>4244.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>104924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>18237.0</td>\n",
       "      <td>8904.0</td>\n",
       "      <td>15889.0</td>\n",
       "      <td>14293.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>21068.0</td>\n",
       "      <td>7769.0</td>\n",
       "      <td>2726.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>104781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP     ENFP  \\\n",
       "i     67871.0  27403.0  52115.0  43864.0  8881.0  8687.0  87712.0  31198.0   \n",
       "the   39663.0  18994.0  35868.0  30500.0  6135.0  5019.0  48008.0  16454.0   \n",
       "to    40231.0  17852.0  33005.0  28753.0  5889.0  5471.0  48996.0  16945.0   \n",
       "a     31932.0  14728.0  26693.0  22780.0  4748.0  3966.0  40376.0  13847.0   \n",
       "and   31628.0  14236.0  24881.0  21568.0  4564.0  4343.0  40710.0  15002.0   \n",
       "of    24312.0  11335.0  21372.0  17857.0  3499.0  3114.0  29576.0  10217.0   \n",
       "you   22221.0  10882.0  17197.0  16010.0  3815.0  3050.0  24971.0  10329.0   \n",
       "that  19445.0   8949.0  16396.0  14436.0  2907.0  2614.0  23445.0   8591.0   \n",
       "it    18376.0   8265.0  15708.0  13179.0  2604.0  2280.0  22537.0   8028.0   \n",
       "is    18237.0   8904.0  15889.0  14293.0  3000.0  2404.0  21068.0   7769.0   \n",
       "\n",
       "         ISFP     ISTP    ISFJ    ISTJ    ESTP    ESFP    ESTJ    ESFJ     all  \n",
       "i     11148.0  13883.0  8044.0  8172.0  3704.0  1696.0  1856.0  2168.0  378402  \n",
       "the    6131.0   8893.0  4111.0  5143.0  2191.0   937.0  1000.0  1200.0  230247  \n",
       "to     6264.0   8725.0  4607.0  5106.0  2254.0   972.0  1078.0  1223.0  227371  \n",
       "a      4825.0   7124.0  3333.0  4033.0  1868.0   796.0   841.0   986.0  182876  \n",
       "and    5153.0   6540.0  3571.0  3827.0  1905.0   834.0   943.0   988.0  180693  \n",
       "of     3580.0   4962.0  2475.0  2976.0  1300.0   557.0   650.0   779.0  138561  \n",
       "you    3332.0   4696.0  2186.0  2734.0  1396.0   651.0   653.0   639.0  124762  \n",
       "that   2931.0   4055.0  2033.0  2207.0  1063.0   550.0   521.0   615.0  110758  \n",
       "it     2909.0   4244.0  2046.0  2290.0  1065.0   434.0   469.0   490.0  104924  \n",
       "is     2726.0   3704.0  1879.0  2186.0  1121.0   482.0   554.0   565.0  104781  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MTEdKrZVAA8"
   },
   "source": [
    "This isn't very helpful at all, is it? It's very difficult to extract insights from this data.  Let's see if we can use the $chi^2$ test to see whether Introverts favour the word **`I`**. We'll do this first by extracting introvert types only from all the personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eTstsUzVAA8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "intro_types = [p for p in type_labels if p[0] == 'I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an introvert total word count column, which sums the counts of all introvert columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F3a0V80VAA_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_bow['I'] = df_bow[intro_types].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate and add percentage columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tESewJzZVABA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for col in ['I', 'all']:\n",
    "    df_bow[col+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print off the dataframe to view what we've done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "      <th>I</th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>67871.0</td>\n",
       "      <td>27403.0</td>\n",
       "      <td>52115.0</td>\n",
       "      <td>43864.0</td>\n",
       "      <td>8881.0</td>\n",
       "      <td>8687.0</td>\n",
       "      <td>87712.0</td>\n",
       "      <td>31198.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>8172.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>378402</td>\n",
       "      <td>292809.0</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.047348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>39663.0</td>\n",
       "      <td>18994.0</td>\n",
       "      <td>35868.0</td>\n",
       "      <td>30500.0</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>5019.0</td>\n",
       "      <td>48008.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>5143.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>230247</td>\n",
       "      <td>178317.0</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.028810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>40231.0</td>\n",
       "      <td>17852.0</td>\n",
       "      <td>33005.0</td>\n",
       "      <td>28753.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>5106.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>227371</td>\n",
       "      <td>175687.0</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.028450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>31932.0</td>\n",
       "      <td>14728.0</td>\n",
       "      <td>26693.0</td>\n",
       "      <td>22780.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>40376.0</td>\n",
       "      <td>13847.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>182876</td>\n",
       "      <td>141096.0</td>\n",
       "      <td>0.022995</td>\n",
       "      <td>0.022883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24881.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40710.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180693</td>\n",
       "      <td>137878.0</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.022610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP     ENFP  \\\n",
       "i    67871.0  27403.0  52115.0  43864.0  8881.0  8687.0  87712.0  31198.0   \n",
       "the  39663.0  18994.0  35868.0  30500.0  6135.0  5019.0  48008.0  16454.0   \n",
       "to   40231.0  17852.0  33005.0  28753.0  5889.0  5471.0  48996.0  16945.0   \n",
       "a    31932.0  14728.0  26693.0  22780.0  4748.0  3966.0  40376.0  13847.0   \n",
       "and  31628.0  14236.0  24881.0  21568.0  4564.0  4343.0  40710.0  15002.0   \n",
       "\n",
       "        ISFP     ISTP    ISFJ    ISTJ    ESTP    ESFP    ESTJ    ESFJ     all  \\\n",
       "i    11148.0  13883.0  8044.0  8172.0  3704.0  1696.0  1856.0  2168.0  378402   \n",
       "the   6131.0   8893.0  4111.0  5143.0  2191.0   937.0  1000.0  1200.0  230247   \n",
       "to    6264.0   8725.0  4607.0  5106.0  2254.0   972.0  1078.0  1223.0  227371   \n",
       "a     4825.0   7124.0  3333.0  4033.0  1868.0   796.0   841.0   986.0  182876   \n",
       "and   5153.0   6540.0  3571.0  3827.0  1905.0   834.0   943.0   988.0  180693   \n",
       "\n",
       "            I    I_perc  all_perc  \n",
       "i    292809.0  0.047721  0.047348  \n",
       "the  178317.0  0.029061  0.028810  \n",
       "to   175687.0  0.028633  0.028450  \n",
       "a    141096.0  0.022995  0.022883  \n",
       "and  137878.0  0.022471  0.022610  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrpDw6ufVABC"
   },
   "source": [
    "Do you remember the $chi^2$ test from the CINDY framework?  This looks at observed versus expected results and lets us know where the greatest differences from expected values are.  The bigger the statistic, the greater the difference from expectation.  The formula is \n",
    "\n",
    "$$𝑐ℎ𝑖^2 = \\sum{\\frac{(𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 −𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑)^2}{𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vguk_YpZVABC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# calculate chi2\n",
    "df_bow['chi2'] = np.power((df_bow['I_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOesPcaXVABD",
    "outputId": "b973bca5-2d74-4cdd-9a71-fc9956c0914f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infp</th>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infj</th>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infps</th>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infjs</th>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intp</th>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.012052</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intps</th>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.012355</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I_perc  all_perc      chi2\n",
       "urlweb  0.002816  0.002614  0.000016\n",
       "infp    0.001249  0.001118  0.000015\n",
       "infj    0.001111  0.001019  0.000008\n",
       "infps   0.000465  0.000413  0.000007\n",
       "infjs   0.000393  0.000347  0.000006\n",
       "intp    0.000936  0.000871  0.000005\n",
       "my      0.012052  0.011854  0.000003\n",
       "intps   0.000346  0.000315  0.000003\n",
       "i       0.047721  0.047348  0.000003\n",
       "in      0.012355  0.012171  0.000003"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] > df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dySRM0KqVABE"
   },
   "source": [
    "And there it is! What can we conclude from this:\n",
    "* `I` is the 9th most introverted word, by expectation\n",
    "* Introverts tend to post more urls than extroverted people! \n",
    "* The introverted types are more likely to be written by Introverts, maybe because people post about their own personality types?\n",
    "\n",
    "Let's now have a look at the words most used by extroverts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LKZqPxRVABF",
    "outputId": "a13add7d-2344-43e5-95cf-2cfcbadf01e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entp</th>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entps</th>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfps</th>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entj</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfj</th>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estp</th>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entjs</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfjs</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.015213</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w6</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w8</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         I_perc  all_perc      chi2\n",
       "enfp   0.000454  0.000728  0.000103\n",
       "entp   0.000376  0.000631  0.000103\n",
       "entps  0.000113  0.000226  0.000057\n",
       "enfps  0.000129  0.000228  0.000043\n",
       "entj   0.000245  0.000360  0.000036\n",
       "enfj   0.000273  0.000356  0.000019\n",
       "estp   0.000219  0.000289  0.000017\n",
       "entjs  0.000064  0.000105  0.000016\n",
       "d      0.000358  0.000428  0.000011\n",
       "enfjs  0.000072  0.000106  0.000011\n",
       "ne     0.000254  0.000312  0.000011\n",
       "you    0.015213  0.015611  0.000010\n",
       "7w6    0.000019  0.000038  0.000010\n",
       "7w8    0.000011  0.000027  0.000009\n",
       "he     0.002386  0.002532  0.000008"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] < df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we conclude from this:\n",
    "* `you` is the 12th most extroverted word. \n",
    "* The extroverted types are more likely to be written by Extroverts, again showing that people post about their own personality types.\n",
    "\n",
    "These suggest that, in these posts, extroverts would be interacting with other people while introverts seem to discuss themselves, their interests or experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TlO1q-zlVABg"
   },
   "source": [
    "### ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXZa-xFeVABh"
   },
   "source": [
    "While individual words do carry meaning, it is often the case that combinations of words change meanings of sentences entirely.  For example, what difference does removing the `not` from this sentence make?\n",
    "\n",
    "Natural Language Processing is **not** easy!\n",
    "\n",
    "ngrams are a method to extract combinations of words into features for model buildiing.  The `n` in ngrams specifies the number of tokens to include.  For example, a 2-gram returns all the consecutive pairs of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGJEya0iVABi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-W1NOt4VABi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def word_grams(words, min_n=1, max_n=4):\n",
    "    s = []\n",
    "    for n in range(min_n, max_n):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uyHBnmoVABj",
    "outputId": "c5408f67-e2e2-40a6-a7c4-e31c5c359853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'one two', 'two three', 'three four', 'one two three', 'two three four']\n"
     ]
    }
   ],
   "source": [
    "print (word_grams('one two three four'.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine consecutive words into groups of 2 using ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jR-KSHUVABm",
    "outputId": "b2a4e948-4a7b-4c86-9a98-b2f530bd3174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'find'),\n",
       " ('find', 'all'),\n",
       " ('all', 'of'),\n",
       " ('of', 'you'),\n",
       " ('you', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'extremely'),\n",
       " ('extremely', 'humorous'),\n",
       " ('humorous', 'now'),\n",
       " ('now', 'to'),\n",
       " ('to', 'find'),\n",
       " ('find', 'other'),\n",
       " ('other', 'specimen'),\n",
       " ('specimen', 'to'),\n",
       " ('to', 'observe')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[55555]['tokens'], 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine consecutive words into groups of 3 using ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVUBu-36VABo",
    "outputId": "882045f8-7e10-4f47-d939-bf127459d543"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'find', 'all'),\n",
       " ('find', 'all', 'of'),\n",
       " ('all', 'of', 'you'),\n",
       " ('of', 'you', 'to'),\n",
       " ('you', 'to', 'be'),\n",
       " ('to', 'be', 'extremely'),\n",
       " ('be', 'extremely', 'humorous'),\n",
       " ('extremely', 'humorous', 'now'),\n",
       " ('humorous', 'now', 'to'),\n",
       " ('now', 'to', 'find'),\n",
       " ('to', 'find', 'other'),\n",
       " ('find', 'other', 'specimen'),\n",
       " ('other', 'specimen', 'to'),\n",
       " ('specimen', 'to', 'observe')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[55555]['tokens'], 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sm4dVWzVABG"
   },
   "source": [
    "## Now that we understand all of that, let's cheat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUvfk_qrVABG"
   },
   "source": [
    "Praise be to Python...\n",
    "\n",
    "`sklearn` has a built in text feature extraction module called [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will literally do all of that work in one line of code! This function will convert a collection of documents (rows of text) into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTbEo8uEVABH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fe57kvsHVABI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFzCFS89VABM"
   },
   "source": [
    "### Tuning the vectorizer\n",
    "\n",
    "We have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune with examples on how to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqeL2KJ4VABR"
   },
   "source": [
    "- **stop_words:** string 'english', list, or None (default)\n",
    "    * If 'english', a built-in stop word list for English is used.\n",
    "    * If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    * If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzEOoIt0VABR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WglDIhfhVABU"
   },
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wx1TyjjVABU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfz8TkhbVABW"
   },
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5B_nbBDVABW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CMv962vVABY"
   },
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8OcjOK9VABZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GTogGUiVABa"
   },
   "source": [
    "### Guidelines for tuning CountVectorizer:\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "- **Experiment**, and let the data tell you the best approach!\n",
    "\n",
    "Finally, let's fit a tuned CountVectorizer to the MBTI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTPnl452VABa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsB226qGVABc",
    "outputId": "d68d31e5-b3d4-4a3f-bb91-97724bf9521e"
   },
   "outputs": [],
   "source": [
    "betterVect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we covered various techniques for cleaning text data and extracting features to use with machine learning models. We also demonstrated how NLTK's `CountVectorizer` can be used to clean text data and extract features, transforming the text data into a matrix of numbers that can be fed into a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [NLTK](http://www.nltk.org/)\n",
    "* [Tokenisation](http://www.nltk.org/howto/tokenize.html) \n",
    "* [Stemming](http://www.nltk.org/howto/stem.html)\n",
    "* [Lemmatization](https://pythonprogramming.net/lemmatizing-nltk-tutorial/)\n",
    "* [ngrams](http://www.nltk.org/api/nltk.html?highlight=n%20grams#nltk.util.ngrams)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VCXae5QXU__Z",
    "wzM8TbWBU__h",
    "FvA-QZmRU__r",
    "hAUkklVXU__6",
    "rFln-NFtVAAI",
    "UZomXVzoVAAR",
    "qp-n688CVAAc",
    "tGmGzrbsVAAf",
    "oFzCFS89VABM",
    "TlO1q-zlVABg"
   ],
   "name": "3_How-do-machines-understand language.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
